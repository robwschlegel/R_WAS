# Hypothesis Testing {#hypo}  
    
```{r setup_hypo, include=FALSE, purl=FALSE}
chap <- 7
lc <- 0
rq <- 0
# **`r paste0("(LC", chap, ".", (lc <- lc + 1), ")")`**
# **`r paste0("(RQ", chap, ".", (rq <- rq + 1), ")")`**
knitr::opts_chunk$set(
  tidy = FALSE, 
  out.width = '\\textwidth', 
  fig.height = 4,
  warning = FALSE
  )

# if(!file.exists("rds/simGuesses.rds")){
#   simGuesses <- do(5000) * rflip(10)
#   saveRDS(object = simGuesses, "rds/simGuesses.rds")
# } else {
#   simGuesses <- readRDS("rds/simGuesses.rds")
# }

```

We saw some of the main concepts of hypothesis testing introduced in Chapter \@ref(sim).  We will expand further on these ideas here and also provide a framework for understanding hypothesis tests in general.  Instead of presenting you with lots of different formulas and scenarios, we hope to build a way to think about all hypothesis tests.  You can then adapt to different scenarios as needed down the road when you encounter different statistical situations.

The same can be said for confidence intervals.  There is one general framework that applies to all confidence intervals and we will elaborate on this further in Chapter \@ref(ci).  The specifics may change slightly for each variation, but the important idea is to understand the general framework so that you can apply it to more specific problems.  We believe that this approach is much better in the long-term than teaching you specific tests and confidence intervals rigorously.  You can find fully-worked out examples for five common hypothesis tests and their corresponding confidence intervals in Appendix \@ref(appendixB).  

We recommend that you carefully review these examples as they also cover how the general frameworks apply to traditional normal-based methodologies like the $t$-test and normal-theory confidence intervals.  You'll see there that these methods are just approximations for the general computational frameworks, but require conditions to be met for their results to be valid.  The general frameworks using randomization, simulation, and bootstrapping do not hold the same sorts of restrictions and further advance computational thinking, which is one big reason for their emphasis throughout this textbook.

### Needed packages {-}

```{r message=FALSE, warning=FALSE}
library(dplyr)
library(ggplot2)
library(mosaic)
library(knitr)
library(nycflights13)
library(ggplot2movies)
```

## When Inference Is Not Needed

Before we delve into the two techniques of inference (hypothesis testing and confidence intervals), it's good to remember that there are cases where you need not perform a rigorous statistical inference.  An important and time-saving skill is to **ALWAYS** do exploratory data analysis using `dplyr` and `ggplot2` before thinking about running a hypothesis test.  Let's look at such an example selecting a sample of flights traveling to Boston and to San Francisco from New York City in the `flights` data frame in the `nycflights13` package.  (We will remove flights with missing data first using `na.omit` and then sample 100 flights going to each of the two airports.)

```{r}
library(nycflights13)
data(flights)
bos_sfo <- flights %>% na.omit() %>% 
  filter(dest %in% c("BOS", "SFO")) %>% 
  group_by(dest) %>% 
  sample_n(100)
```

Suppose we were interested in seeing if the `air_time` to SFO in San Francisco was statistically greater than the `air_time` to BOS in Boston.  As suggested, let's begin with some exploratory data analysis to get a sense for how the two variables of `air_time` and `dest` relate for these two destination airports:

```{r}
library(dplyr)
bos_sfo_summary <- bos_sfo %>% group_by(dest) %>% 
  summarize(mean_time = mean(air_time),
            sd_time = sd(air_time))
kable(bos_sfo_summary)
```

Looking at these results, we can clearly see that SFO `air_time` is much larger than BOS `air_time`.  The standard deviation is also extremely informative here.

***
```{block lc6-2b, type='learncheck', purl=FALSE}
**_Learning check_**
```

**`r paste0("(LC", chap, ".", (lc <- lc + 1), ")")`** Could we make the same type of immediate conclusion that SFO had a statistically greater `air_time` if, say, its corresponding standard deviation was 200 minutes?  What about 100 minutes?  Explain.

***

To further understand just how different the `air_time` variable is for BOS and SFO, let's look at a boxplot:

```{r}
library(ggplot2)
ggplot(data = bos_sfo, mapping = aes(x = dest, y = air_time)) +
  geom_boxplot()
```

Since there is no overlap at all, we can conclude that the `air_time` for San Francisco flights is statistically greater (at any level of significance) than the `air_time` for Boston flights.  This is a clear example of not needing to do anything more than some simple descriptive statistics to get an appropriate inferential conclusion.  This is one reason why you should **ALWAYS** investigate the sample data first using `dplyr` and `ggplot2` via exploratory data analysis. 
  
As you get more and more practice with hypothesis testing, you'll be better able to determine in many cases whether or not the results will be statistically significant.  There are circumstances where it is difficult to tell, but you should always try to make a guess FIRST about significance after you have completed your data exploration and before you actually begin the inferential techniques.  

## Basics of Hypothesis Testing

In a hypothesis test, we will use data from a sample to help us decide between two competing _hypotheses_ about a population.  We make these hypotheses more concrete by specifying them in terms of at least one _population parameter_ of interest.  We refer to the competing claims about the population as the **null hypothesis**, denoted by $H_0$, and the **alternative (or research) hypothesis**, denoted by $H_a$.  The roles of these two hypotheses are NOT interchangeable.  

- The claim for which we seek significant evidence is assigned to the alternative hypothesis.  The alternative is usually what the experimenter or researcher wants to establish or find evidence for.
- Usually, the null hypothesis is a claim that there really is "no effect" or "no difference."  In many cases, the null hypothesis represents the status quo or that nothing interesting is happening.  
- We assess the strength of evidence by assuming the null hypothesis is true and determining how unlikely it would be to see sample results/statistics as extreme (or more extreme) as those in the original sample.

Hypothesis testing brings about many weird and incorrect notions in the scientific community and society at large.  One reason for this is that statistics has traditionally been thought of as this magic box of algorithms and procedures to get to results and this has been readily apparent if you do a Google search of "flowchart statistics hypothesis tests".  There are so many different complex ways to determine which test is appropriate.  

You'll see that we don't need to rely on these complicated series of assumptions and procedures to conduct a hypothesis test any longer.  These methods were introduced in a time when computers weren't powerful.  Your cellphone (in 2016) has more power than the computers that sent NASA astronauts to the moon after all.  We'll see that ALL hypothesis tests can be broken down into the following framework given by Allen Downey [here](http://allendowney.blogspot.com/2016/06/there-is-still-only-one-test.html):

```{r htdowney, echo=FALSE, fig.cap="Hypothesis Testing Framework", purl=FALSE}
knitr::include_graphics("images/ht.png")
```

Before we hop into this framework, we will provide another way to think about hypothesis testing that may be useful.

## Criminal trial analogy {#trial}

We can think of hypothesis testing in the same context as a criminal trial in the United States.  A criminal trial in the United States is a familiar situation in which a choice between two contradictory claims must be made. 

1. The accuser of the crime must be judged either guilty or not guilty.  

2. Under the U.S. system of justice, the individual on trial is initially presumed not guilty.  

3. Only STRONG EVIDENCE to the contrary causes the not guilty claim to be rejected in favor of a guilty verdict. 

4. The phrase "beyond a reasonable doubt" is often used to set the cutoff value for when enough evidence has been given to convict.

Theoretically, we should never say "The person is innocent." but instead "There is not sufficient evidence to show that the person is guilty."

Now let's compare that to how we look at a hypothesis test.

1. The decision about the population parameter(s) must be judged to follow one of two hypotheses.
	
2. We initially assume that $H_0$ is true.
	
3. The null hypothesis $H_0$ will be rejected (in favor of $H_a$) only if the sample evidence strongly suggests that $H_0$ is false.  If the sample does not provide such evidence, $H_0$ will not be rejected.

4.  The analogy to "beyond a reasonable doubt" in hypothesis testing is what is known as the **significance level**.  This will be set before conducting the hypothesis test and is denoted as $\alpha$.  Common values for $\alpha$ are 0.1, 0.01, and 0.05.

### Two possible conclusions

Therefore, we have two possible conclusions with hypothesis testing:

 - Reject $H_0$                
 - Fail to reject $H_0$
	
Gut instinct says that "Fail to reject $H_0$" should say "Accept $H_0$" but this technically is not correct.  Accepting $H_0$ is the same as saying that a person is innocent.  We cannot show that a person is innocent; we can only say that there was not enough substantial evidence to find the person guilty.

When you run a hypothesis test, you are the jury of the trial.  You decide whether there is enough evidence to convince yourself that $H_a$ is true ("the person is guilty") or that there was not enough evidence to convince yourself $H_a$ is true ("the person is not guilty").  You must convince yourself (using statistical arguments) which hypothesis is the correct one given the sample information.

**Important note:** Therefore, DO NOT WRITE "Accept $H_0$" any time you conduct a hypothesis test.  Instead write "Fail to reject $H_0$."

## Types of Errors in Hypothesis Testing

Unfortunately, just as a jury or a judge can make an incorrect decision in regards to a criminal trial by reaching the wrong verdict, there is some chance we will reach the wrong conclusion via a hypothesis test about a population parameter.  As with criminal trials, this comes from the fact that we don't have complete information, but rather a sample from which to try to infer about a population.

The possible erroneous conclusions in a criminal trial are

- an innocent person is convicted (found guilty) or
- a guilty person is set free (found not guilty).

The possible errors in a hypothesis test are

- rejecting $H_0$ when in fact $H_0$ is true (Type I Error) or
- failing to reject $H_0$ when in fact $H_0$ is false (Type II Error).

The risk of error is the price researchers pay for basing an inference about a population on a sample.  With any reasonable sample-based procedure, there is some chance that a Type I error will be made and some chance that a Type II error will occur.

To help understand the concepts of Type I error and Type II error, observe the following table:

```{r, echo=FALSE, fig.cap="Type I and Type II errors", purl=FALSE}
knitr::include_graphics("images/errors.png")
```

If we are using sample data to make inferences about a parameter, we run the risk of making a mistake.  Obviously, we want to minimize our chance of error; we want a small probability of drawing an incorrect conclusion.

- The probability of a Type I Error occurring is denoted by $\alpha$ and is called the **significance level** of a hypothesis test
- The probability of a Type II Error is denoted by $\beta$.

Formally, we can define $\alpha$ and $\beta$ in regards to the table above, but for hypothesis tests instead of a criminal trial.

- $\alpha$ corresponds to the probability of rejecting $H_0$ when, in fact, $H_0$ is true.
- $\beta$ corresponds to the probability of failing to reject $H_0$ when, in fact, $H_0$ is false.

Ideally, we want $\alpha = 0$ and $\beta = 0$, meaning that the chance of making an error does not exist.  When we have to use incomplete information (sample data), it is not possible to have both $\alpha = 0$ and $\beta = 0$.  We will always have the possibility of at least one error existing when we use sample data.

Usually, what is done is that $\alpha$ is set before the hypothesis test is conducted and then the evidence is judged against that significance level.  Common values for $\alpha$ are 0.05, 0.01, and 0.10.  If $\alpha = 0.05$, we are using a testing procedure that, used over and over with different samples, rejects a TRUE null hypothesis five percent of the time.

So if we can set $\alpha$ to be whatever we want, why choose 0.05 instead of 0.01 or even better 0.0000000000000001?  Well, a small $\alpha$ means the test procedure requires the evidence against $H_0$ to be **very strong** before we can reject $H_0$.  This means we will almost never reject $H_0$ if $\alpha$ is very small.  If we almost never reject $H_0$, the probability of a Type II Error -- failing to reject $H_0$ when we should -- will *increase*!  Thus, as $\alpha$ decreases, $\beta$ increases and as $\alpha$ increases, $\beta$ decreases.  We, therefore, need to strike a balance in $\alpha$ and $\beta$ and the common values for $\alpha$ of 0.05, 0.01, and 0.10 usually lead to a nice balance.

***

```{block lc7-0, type='learncheck', purl=FALSE}
**_Learning check_**
```

**`r paste0("(LC", chap, ".", (lc <- lc + 1), ")")`**  Reproduce the table above about errors, but for a hypothesis test, instead of the one provided for a criminal trial.

***

### Logic of Hypothesis Testing

- Take a random sample (or samples) from a population (or multiple populations)
- If the sample data are consistent with the null hypothesis, do not reject the null hypothesis.
- If the sample data are inconsistent with the null hypothesis (in the direction of the alternative hypothesis), reject the null hypothesis and conclude that there is evidence the alternative hypothesis is true (based on the particular sample collected).

## Statistical Significance

The idea that sample results are more extreme than we would reasonably expect to see by random chance if the null hypothesis were true is the fundamental idea behind statistical hypothesis tests.  If data at least as extreme would be very unlikely if the null hypothesis were true, we say the data are **statistically significant**.  Statistically significant data provide convincing evidence against the null hypothesis in favor of the alternative, and allow us to generalize our sample results to the claim about the population.

```{block lc7-1, type='learncheck', purl=FALSE}
**_Learning check_**
```

**`r paste0("(LC", chap, ".", (lc <- lc + 1), ")")`**  What is wrong about saying "The defendant is innocent." based on the US system of criminal trials?

**`r paste0("(LC", chap, ".", (lc <- lc + 1), ")")`**  What is the purpose of hypothesis testing?

**`r paste0("(LC", chap, ".", (lc <- lc + 1), ")")`**  What are some flaws with hypothesis testing?  How could we alleviate them?

***


## EXAMPLE: Revisiting the Lady Tasting Tea

Recall the "There is Only One Test" diagram from earlier:

```{r htdowney2, echo=FALSE, fig.cap="Hypothesis Testing Framework", purl=FALSE}
knitr::include_graphics("images/ht.png")
```

We will now walk through how each of the steps to the diagram apply to determining whether the lady tasting tea was actually better than chance at determining whether or not milk was added first.  We will see that the process of creating a null distribution is a statistical way to quantifying surprise.

### Data

Let's assume as we did in Chapter \@ref(sim) that the lady is correct in determining whether milk was added first or not in 9 out of 10 trials.  Our data, therefore, may look something like

```{r echo=FALSE}
choice <- c(rep("Correct", 3), "Incorrect", rep("Correct", 6))
kable(choice)
```

### Test Statistic $\delta$

We are interested in the number of `Correct` out of our 10 trials.  We can denote this number of successes using the symbol $t$, where $t$ corresponds to total.  This is our test statistic $\delta$ in this case.

### Observed effect $\delta^*$

The actual observed value of the test statistic from our observed sample is $\hat{t}_{obs} = 9$.  Thus, $\delta^* = 9$.

### Model of $H_0$

Our null hypothesis is that the lady is only as good as chance at guessing correctly.  Hypotheses always correspond to parameters and are denoted with Greek letters.  Thus, symbolically, we have $H_0: \tau = 5$.  Since we are assuming chance and we have 10 flips with 0.5 probability of success of each flip, we have $\tau = 10 \times 0.5 = 5$.

### Simulated Data

We now want to use this null hypothesis to simulate the test statistic assuming that the null hypothesis is true.  Therefore, we want to figure out a way to simulate 10 trials, getting either the choice Correct or Incorrect, assuming that the probability of success (getting it Correct) in any given trial is 0.5.

**Tactile simulation**

When you are presented with a hypothesis testing problem, frequently the most challenging portion is setting up how to simulate the data assuming the null hypothesis is true.  To facilitate with this, setting up a tactile, hands on experiment can help.

In this case, flipping a fair coin is a great way to simulate this process.  This simulates how the sample could be collected assuming the null hypothesis is true.  To simulate 10 trials, we could flip the fair coin and record Heads as Correct and Tails as Incorrect.  

Some simulated data using this coin flipping procedure may look like the following.  Note that this data frame is not tidy, but is a convenient way to look at the results of the simulation in this wide format.  The numbers on the far left correspond to the number of the trial.

```{r sample-table, echo=FALSE}
set.seed(2017)
sim1 <- resample(x = c("Correct", "Incorrect"), size = 10, prob = c(0.5, 0.5))
sim2 <- resample(x = c("Correct", "Incorrect"), size = 10, prob = c(0.5, 0.5))
sim3 <- resample(x = c("Correct", "Incorrect"), size = 10, prob = c(0.5, 0.5))
sims <- data.frame(sample1 = sim1, sample2 = sim2, sample3 = sim3)
kable(sims, row.names = TRUE, caption = 'A table of three sets of 10 coin flips')
```

We then use the formula for the **Test Statistic** to determine the simulated test statistic for each of these simulated samples.  So in this case we have

```{r echo=FALSE}
t1 <- sum(sim1 == "Correct")
t2 <- sum(sim2 == "Correct")
t3 <- sum(sim3 == "Correct")
```

$t_1 = `r t1`$, $t_2 = `r t2`$, $t_3 = `r t3`$

### Distribution of $\delta$ under $H_0$

We could continue this process, say, 5000 times by flipping a coin in sets of 10 for 5000 repetitions and counting and taking note of how many heads out of 10 we have for each set.  It's at this point that you surely realize that a computer can do this procedure much faster
and more efficient than the tactile experiment with a coin.

Recall that we've already created the distribution of 5000 such coin flips and we've stored these values in the `heads` variable in the `simGuesses` data frame:

```{r}
library(ggplot2)
ggplot(data = simGuesses, aes(x = factor(heads))) +
  geom_bar()
```


### The p-value

---

**Definition: $p$-value**:

The **p-value** is the probability of observing a sample statistic as extreme or more extreme than what was observed, assuming that the null hypothesis of a by chance operation is true.

---

This definition may be a little intimidating the first time you read it, but it's important to come back to this "The Lady Tasting Tea" problem whenever you encounter $p$-values as you begin to learn about the concept.  Here the $p$-value corresponds to how many times in our **null distribution** of `heads` 9 or more heads occurred.

We can use another neat feature of R to calculate the $p$-value for this problem.  Note that "more extreme" in this case corresponds to looking at values of 9 or greater since our alternative hypothesis invokes a right-tail test corresponding to a "greater than" hypothesis of $H_a: \tau > 5$.  In other words, we are looking to see how likely it is for the lady to pick 9 or more correct instead of 9 or less correct.  We'd like to go in the right direction.

```{r}
pvalue_tea <- simGuesses %>%
  filter(heads >= 9) %>%
  nrow() / nrow(simGuesses)
```

Let's walk through each step of this calculation:

1. First, `pvalue_tea` will be the name of our calculated $p$-value and the assignment operator `<-` directs us to this naming.

2. We are working with the `simGuesses` data frame here so that comes immediately before the pipe operator.  
3. We would like to only focus on the rows in our `simGuesses` data frame that have `heads` values of 9 or 10.  This represents simulated statistics "as extreme or more extreme" than what we observed (9 correct guesses out of 10).  To get a glimpse of what we have up to this point, run `simGuesses %>% filter(heads >= 9) %>% View()`.

4. Now that we have changed the focus to only those rows that have number of heads out of 10 flips corresponding to 9 or more, we count how many of those there are.  The function `nrow` gives how many entries are in this filtered data frame and lastly we calculate the proportion that are at least as extreme as our observed value of 9 by dividing by the number of total simulations (`r format(nrow(simGuesses), big.mark = ",")`).

We can see that the observed statistic of 9 correct guesses is not a likely outcome assuming the null hypothesis is true.  Only around 1% of the outcomes in our 5000 simulations fall at or above 9 successes.  We have evidence supporting the conclusion that the person is actually better than just guessing at random at determining whether milk has been added first or not.  To better visualize this we can also make use of blue shading on the histogram corresponding to the $p$-value:

```{r fig.cap="Barplot of heads with p-value highlighted"}
library(ggplot2)
ggplot(data = simGuesses, aes(x = factor(heads), fill = (heads >= 9))) +
  geom_bar() +
  labs(x = "heads")
```

This helps us better see just how few of the values of `heads` are at our observed value or more extreme.  This idea of a $p$-value can be extended to the more traditional methods using normal and $t$ distributions in the traditional way that introductory statistics has been presented.  These traditional methods were used because statisticians haven't always been able to do 5000 simulations on the computer within seconds.  We'll elaborate on this more in a few sections.

***
```{block lc6-2, type='learncheck', purl=FALSE}
**_Learning check_**
```

**`r paste0("(LC", chap, ".", (lc <- lc + 1), ")")`** How could we make Table \@ref(tab:sample-table) into a tidy data frame?

**`r paste0("(LC", chap, ".", (lc <- lc + 1), ")")`** What is meant by "pseudo-random number generation?"

**`r paste0("(LC", chap, ".", (lc <- lc + 1), ")")`** How can simulation be used to help us address the question of whether or not an observed result is statistically significant?

**`r paste0("(LC", chap, ".", (lc <- lc + 1), ")")`** In Chapter \@ref(viz), we noted that barplots should be used when creating a plot of categorical variables.  Why are we using barplots to make a plot of a numerical variable `heads` in this chapter?

***

## EXAMPLE: Comparing two means

### Randomization/Permutation

We will now focus on building hypotheses looking at the difference between two population means in an example.  We will denote population means using the Greek symbol $\mu$ (pronounced "mu").  Thus, we will be looking to see if one group "out-performs" another group.  This is quite possibly the most common type of statistical inference and serves as a basis for many other types of analyses when comparing the relationship between two variables. 

Our null hypothesis will be of the form $H_0: \mu_1 = \mu_2$, which can also be written as $H_0: \mu_1 - \mu_2 = 0$.  Our alternative hypothesis will be of the form $H_0: \mu_1 \star \mu_2$ (or $H_a:  \mu_1 - \mu_2 \, \star \, 0$) where $\star$ = $<$, $\ne$, or $>$ depending on the context of the problem.  You needn't focus on these new symbols too much at this point.  It will just be a shortcut way for us to describe our hypotheses.

As we saw earlier, simulation is a valuable tool when conducting inferences based on one population variable.  We will see that the process of **randomization** (also known as **permutation**) will be valuable in conducting tests comparing quantitative values from two groups.

### Comparing Action and Romance Movies

The `movies` data set in the `ggplot2movies` package contains information on a large number of movies that have been rated by users of IMDB.com [@R-ggplot2movies].  We are interested in the question here of whether `Action` movies are rated higher on IMDB than `Romance` movies.  We will first need to do a little bit of data manipulation using the ideas from Chapter \@ref(manip) to get the data in the form that we would like:

```{r message=FALSE, warning=FALSE}
library(dplyr)
library(ggplot2movies)
(movies_trimmed <- movies %>% select(title, year, rating, Action, Romance))
```

Note that `Action` and `Romance` are binary variables here.  To remove any overlap of movies (and potential confusion) that are both `Action` and `Romance`, we will remove them from our _population_:

```{r}
movies_trimmed <- movies_trimmed %>%
  filter(!(Action == 1 & Romance == 1))
```

We will now create a new variable called `genre` that specifies whether a movie in our `movies_trimmed` data frame is an `"Action"` movie, a `"Romance"` movie, or `"Neither"`.  We aren't really interested in the `"Neither"` category here so we will exclude those rows as well.  Lastly, the `Action` and `Romance` columns are not needed anymore since they are encoded in the `genre` column.

```{r}
movies_trimmed <- movies_trimmed %>%
  mutate(genre = ifelse(Action == 1, "Action",
                        ifelse(Romance == 1, "Romance",
                               "Neither"))) %>%
  filter(genre != "Neither") %>%
  select(-Action, -Romance)
```
  
We are left with `r nrow(movies_trimmed)` movies in our _population_ data set that focuses on only `"Action"` and `"Romance"` movies.  
    
***
```{block lc7-2, type='learncheck', purl=FALSE}
**_Learning check_**
```

**`r paste0("(LC", chap, ".", (lc <- lc + 1), ")")`** Why are the different genre variables stored as binary variables (1s and 0s) instead of just listing the `genre` as a column of values like "Action", "Comedy", etc.?

**`r paste0("(LC", chap, ".", (lc <- lc + 1), ")")`** What complications could come above with us excluding action romance movies?  Should we question the results of our hypothesis test?  Explain.

***    

Let's now visualize the distributions of `rating` across both levels of `genre`.  Think about what type(s) of plot is/are appropriate here before you proceed:

```{r fig.cap="Rating vs genre in the population"}
library(ggplot2)
ggplot(data = movies_trimmed, aes(x = genre, y = rating)) +
  geom_boxplot()
```

We can see that the middle 50% of ratings for `"Action"` movies is more spread out than that of `"Romance"` movies in the population.  `"Romance"` has outliers at both the top and bottoms of the scale though.  We are initially interested in comparing the mean `rating` across these two groups so a faceted histogram may also be useful:

```{r movie-hist, warning=FALSE, fig.cap="Faceted histogram of genre vs rating"}
ggplot(data = movies_trimmed, mapping = aes(x = rating)) +
  geom_histogram(binwidth = 1, color = "white", fill = "dodgerblue") +
  facet_grid(genre ~ .)
```

**Important note:** Remember that we hardly ever have access to the population values as we do here.  This example and the `nycflights13` data set were used to create a common flow from chapter to chapter.  In nearly all circumstances, we'll be needing to use only a sample of the population to try to infer conclusions about the unknown population parameter values.  These examples do show a nice relationship between statistics (where data is usually small and more focused on experimental settings) and data science (where data is frequently large and collected without experimental conditions). 

### Sampling $\rightarrow$ Randomization
    
We can use hypothesis testing to investigate ways to determine, for example, whether a **treatment** has an effect over a **control** and other ways to statistically analyze if one group performs better than, worse than, or different than another.  We will also use confidence intervals to determine the size of the effect, if it exists. You'll see more on this in Chapter \@ref(ci).

We are interested here in seeing how we can use a random sample of action movies and a random sample of romance movies from `movies` to determine if a statistical difference exists in the mean ratings of each group.

***
```{block lc7-3a, type='learncheck', purl=FALSE}
**_Learning check_**
```

**`r paste0("(LC", chap, ".", (lc <- lc + 1), ")")`** Define the relevant parameters here in terms of the populations of movies.

***  

### Data

Let's select a random sample of 34 action movies and a random sample of 34 romance movies.  (The number 34 was chosen somewhat arbitrarily here.)

```{r}
library(dplyr)
library(mosaic)
set.seed(2016)
movies_genre_sample <- movies_trimmed %>% 
  group_by(genre) %>%
  sample_n(34) %>% 
  ungroup()
```

**Note** the addition of the `ungroup()` function here.  This will be useful shortly in allowing us to shuffle the values of `rating` across `genre`.  Our analysis does not work without this `ungroup()` function since the data stays grouped by the levels of `genre` without it.

We can now observe the distributions of our two sample ratings for both groups.  Remember that these plots
should be rough approximations of our population distributions of movie ratings for `"Action"` and `"Romance"`
in our population of all movies in the `movies` data frame.

```{r fig.cap="Genre vs rating for our sample"}
ggplot(data = movies_genre_sample, aes(x = genre, y = rating)) +
  geom_boxplot()
```

```{r warning=FALSE, fig.cap="Genre vs rating for our sample as faceted histogram"}
ggplot(data = movies_genre_sample, mapping = aes(x = rating)) +
  geom_histogram(binwidth = 1, color = "white", fill = "dodgerblue") +
  facet_grid(genre ~ .)
```

***
```{block lc7-3b1, type='learncheck', purl=FALSE}
**_Learning check_**
```

**`r paste0("(LC", chap, ".", (lc <- lc + 1), ")")`** What single value could we change to improve the approximation using the sample distribution on the population distribution?

***

Do we have reason to believe, based on the sample distributions of `rating` over the two groups of `genre`, that there is a significant difference between the mean `rating` for action movies compared to romance movies?  It's hard to say just based on the plots.  The boxplot does show that the median sample rating is higher for romance movies, but the histogram isn't as clear.  The two groups have somewhat differently shaped distributions but they are both over similar values of `rating`.  It's often useful to calculate the mean and standard deviation as well, conditioned on the two levels.

```{r}
summary_ratings <- movies_genre_sample %>% 
  group_by(genre) %>%
  summarize(mean = mean(rating),
            std_dev = sd(rating),
            n = n())
summary_ratings %>% kable()
```

***
```{block lc7-3b2, type='learncheck', purl=FALSE}
**_Learning check_**
```

**`r paste0("(LC", chap, ".", (lc <- lc + 1), ")")`** Why did we not specify `na.rm = TRUE` here as we did in Chapter \@ref(manip)?

***

We see that the sample mean rating for romance movies, $\bar{x}_{r}$, is greater than the similar measure for action movies, $\bar{x}_a$.  But is it statistically significantly greater (thus, leading us to conclude that the means are statistically different)?  The standard deviation can provide some insight here but with these standard deviations being so similar it's still hard to say for sure.

***
```{block lc7-3b3, type='learncheck', purl=FALSE}
**_Learning check_**
```

**`r paste0("(LC", chap, ".", (lc <- lc + 1), ")")`** Why might the standard deviation provide some insight about the means being statistically different or not?

***

### Model of $H_0$

The hypotheses we specified can also be written in another form to better give us an idea of what we will be simulating to create our null distribution.

- $H_0: \mu_r - \mu_a = 0$
- $H_a: \mu_r - \mu_a \ne 0$
  
### Test Statistic $\delta$  
  
We are, therefore, interested in seeing whether the difference in the sample means, $\bar{x}_r - \bar{x}_a$, is statistically different than 0. R has a built-in command that can calculate the difference in these two sample means.

### Observed effect $\delta^*$

```{r}
mean_ratings <- movies_genre_sample %>% 
  group_by(genre) %>%
  summarize(mean = mean(rating))
obs_diff <- diff(mean_ratings$mean)
```

We see here that the `diff` function calculates $\bar{x}_r - \bar{x}_a = `r mean_ratings$mean[2]` - `r mean_ratings$mean[1]` = `r obs_diff`$.  We will now proceed similarly to how we conducted the hypothesis test above for the Lady Tasting Tea using simulation.  Our goal is figure out a random process with which to simulate the null hypothesis being true.  Earlier in this chapter, we used flipping of a fair coin as the random process we were simulating with the null hypothesis being true ($H_0: \tau = 5$).

### Simulated Data

**Tactile simulation**

Here, with us assuming the two population means are equal ($H_0: \mu_r - \mu_a = 0$), we can look at this from a tactile point of view by using index cards.  There are $n_r = 34$ data elements corresponding to romance movies and $n_a = 34$ for action movies.  We can write the 34 ratings from our sample for romance movies on one set of 34 index cards and the 34 ratings for action movies on another set of 34 index cards.  (Note that the sample sizes need not be the same.)

The next step is to put the two stacks of index cards together, creating a new set of 68 cards.  If we assume that the two population means are equal, we are saying that there is no association between ratings and genre (romance vs action).  We can use the index cards to create two **new** stacks for romance and action movies.  First, we must shuffle all the cards thoroughly.  After doing so, in this case with equal values of sample sizes, we split the deck in half.

We then calculate the new sample mean rating of the romance deck, and also the new sample mean rating of the action deck.  This creates one simulation of the samples that were collected originally.  We next want to calculate a statistic from these two samples.  Instead of actually doing the calculation using index cards, we can use R as we have before to simulate this process.

```{r message=FALSE, warning=FALSE}
library(mosaic)
shuffled_ratings <- #movies_trimmed %>%
  movies_genre_sample %>% 
     mutate(rating = shuffle(rating)) %>% 
     group_by(genre) %>%
     summarize(mean = mean(rating))
diff(shuffled_ratings$mean)
```

***
```{block lc7-3b4, type='learncheck', purl=FALSE}
**_Learning check_**
```

**`r paste0("(LC", chap, ".", (lc <- lc + 1), ")")`** How would the tactile shuffling of index cards change if we had different samples of say 20 action movies and 60 romance movies?  Describe each step that would change.

**`r paste0("(LC", chap, ".", (lc <- lc + 1), ")")`** Why are we taking the difference in the means of the cards in the new shuffled decks?

***

### Distribution of $\delta$ under $H_0$

The only new command here is `shuffle` from the `mosaic` package, which does what we would expect it to do.  It simulates a shuffling of the ratings between the two levels of `genre` just as we could have done with index cards.  We can now proceed in a similar way to what we have done previously with the Lady Tasting Tea example by repeating this process many times to create a _null distribution_ of simulated differences in sample means.

```{r include=FALSE}
set.seed(2016)
if(!file.exists("rds/many_shuffles.rds")){
  many_shuffles <- do(5000) * 
    (movies_genre_sample %>% 
      mutate(rating = shuffle(rating)) %>% 
      group_by(genre) %>%
      summarize(mean = mean(rating))
    )
   saveRDS(object = many_shuffles, "rds/many_shuffles.rds")
} else {
   many_shuffles <- readRDS("rds/many_shuffles.rds")
}
```

```{r eval=FALSE}
set.seed(2016)
many_shuffles <- do(5000) * 
  (movies_genre_sample %>% 
     mutate(rating = shuffle(rating)) %>% 
     group_by(genre) %>%
     summarize(mean = mean(rating))
   )
```


It is a good idea here to `View` the `many_shuffles` data frame via `View(many_shuffles)`.  We need to figure out a way to subtract the first value of `mean` from the second value of `mean` for each of the 5000 simulations.  This is a little tricky but the `group_by` function comes to our rescue here:

```{r}
rand_distn <- many_shuffles %>%
  group_by(.index) %>%
  summarize(diffmean = diff(mean))
head(rand_distn, 10)
```

We can now plot the distribution of these simulated differences in means:

```{r fig.cap="Simulated differences in means histogram"}
ggplot(data = rand_distn, aes(x = diffmean)) +
  geom_histogram(color = "white", bins = 20)
```

### The p-value

Remember that we are interested in seeing where our observed sample mean difference of `r diff(mean_ratings$mean)` falls on this null/randomization distribution.  We are interested in simply a difference here so "more extreme" corresponds to values in both tails on the distribution.  Let's shade our null distribution to show a visual representation of our $p$-value:

```{r fig.cap="Shaded histogram to show p-value"}
ggplot(data = rand_distn, aes(x = diffmean, fill = (abs(diffmean) >= obs_diff))) +
  geom_histogram(color = "white", bins = 20)
```

Remember that the observed difference in means was `r obs_diff`.  We have shaded green all values at or above that value and also shaded green those values at or below its negative value (since this is a two-tailed test).  We can add a vertical line to represent both the observed difference and its negative instead.  To better estimate how large the $p$-value will be, we also increase the number of bins to 100 here from 20:

```{r fig.cap="Histogram with vertical lines corresponding to observed statistic"}
ggplot(data = rand_distn, aes(x = diffmean)) +
  geom_histogram(color = "white", bins = 100) +
  geom_vline(xintercept = obs_diff, color = "red") +
  geom_vline(xintercept = -obs_diff, color = "red")
```

At this point, it is important to take a guess as to what the $p$-value may be.  We can see that there are only a few shuffled differences as extreme or more extreme than our observed effect (in both directions).  Maybe we guess that this $p$-value is somewhere around 2%, or maybe 3%, but certainly not 30% or more.  **You'll find yourself getting very strange results if you've messed up the signs in your calculation of the $p$-value so you should always check first that you have a range of reasonable values after looking at the histogram for the $p$-value. Lastly, we calculate the $p$-value directly using `dplyr`:

```{r}
(pvalue_movies <- rand_distn %>%
  filter(abs(diffmean) >= obs_diff) %>%
  nrow() / nrow(rand_distn))
```


We have around `r pvalue_movies * 100`% of values as extreme or more extreme than our observed effect in both directions. Assuming we are using a 5% significance level for $\alpha$, we have evidence supporting the conclusion that the mean rating for romance movies is different from that of action movies.  The next important idea is to better understand just how much higher of a mean rating can we expect the romance movies to have compared to that of action movies.  This can be addressed by creating a 95% confidence interval as we will explore in Chapter \@ref(ci).
  
***

```{block lc7-3b, type='learncheck', purl=FALSE}
**_Learning check_**
```

**`r paste0("(LC", chap, ".", (lc <- lc + 1), ")")`** Conduct the same analysis comparing action movies versus romantic movies using the median rating instead of the mean rating?  Make sure to use the `%>%` as much as possible.  What was different and what was the same? 

**`r paste0("(LC", chap, ".", (lc <- lc + 1), ")")`** What conclusions can you make from viewing the faceted histogram looking at `rating` versus `genre` that you couldn't see when looking at the boxplot?

**`r paste0("(LC", chap, ".", (lc <- lc + 1), ")")`** Describe in a paragraph how we used Allen Downey's diagram to conclude if a statistical difference existed between mean movie ratings for action and romance movies.

**`r paste0("(LC", chap, ".", (lc <- lc + 1), ")")`** Why are we relatively confident that the distributions of the sample ratings will be good approximations of the population distributions of ratings for the two genres?

**`r paste0("(LC", chap, ".", (lc <- lc + 1), ")")`** Using the definition of "$p$-value", write in words what the $p$-value represents for the hypothesis test above comparing the mean rating of romance to action movies.

**`r paste0("(LC", chap, ".", (lc <- lc + 1), ")")`** What is the value of the $p$-value for the hypothesis test comparing the mean rating of romance to action movies?

**`r paste0("(LC", chap, ".", (lc <- lc + 1), ")")`** Do the results of the hypothesis test match up with the original plots we made looking at the population of movies?  Why or why not?

***  

### Summary

To review, these are the steps one would take whenever you'd like to do a hypothesis test comparing
values from the distributions of two groups:

- Simulate many samples using a random process that matches the way
the original data were collected and that _assumes the null hypothesis is
true_. 

- Collect the values of a sample statistic for each sample created using this random process to build
a _randomization distribution_.

- Assess the significance of the _original_ sample by determining where
its sample statistic lies in the randomization distribution.

- If the proportion of values as extreme or more extreme than the observed statistic in the randomization
distribution is smaller than the pre-determined significance level $\alpha$, we reject $H_0$.  Otherwise,
we fail to reject $H_0$.  (If no significance level is given, one can assume $\alpha = 0.05$.)

## Building theory-based methods using computation {#theory-hypo}

As a point of reference, we will now discuss the traditional theory-based way to conduct the hypothesis test for determining if there is a statistically significant difference in the sample mean rating of Action movies versus Romance movies.  This method and ones like it work very well when the assumptions are met in order to run the test.  They are based on probability models and distributions such as the normal and $t$-distributions.  

These traditional methods have been used for many decades back to the time when researchers didn't have access to computers that could run 5000 simulations in under a minute.  They had to base their methods on probability theory instead.  Many fields and researchers continue to use these methods and that is the biggest reason for their inclusion here.  It's important to remember that a $t$-test or a $z$-test is really just an approximation of what you have seen in this chapter already using simulation and randomization.  The focus here is on understanding how the shape of the $t$-curve comes about without digging big into the mathematical underpinnings.

### EXAMPLE:  $t$-test for two independent samples

What is commonly done in statistics is the process of normalization.  What this entails is calculating the mean and standard deviation of a variable.  Then you subtract the mean from each value of your variable and divide by the standard deviation.  The most common normalization is known as the $z$-score.  The formula for a $z$-score is $$Z = \frac{x - \mu}{\sigma},$$ where $x$ represent the value of a variable, $\mu$ represents the mean of the variable, and $\sigma$ represents the standard deviation of the variable.  Thus, if your variable has 10 elements, each one has a corresponding $z$-score that gives how many standard deviations away that value is from its mean.  $z$-scores are normally distributed with mean 0 and standard deviation 1.  They have the common, bell-shaped pattern seen below.

```{r echo=FALSE}
ggplot(data.frame(x = c(-4, 4)), aes(x)) + stat_function(fun = dnorm)
```

Recall, that we hardly ever know the mean and standard deviation of the population of interest.  This is almost always the case when considering the means of two independent groups.  To help account for us not knowing the population parameter values, we can use the sample statistics instead, but this comes with a bit of a price in terms of complexity.

Another form of normalization occurs when we need to use the sample standard deviations as estimates for the unknown population standard deviations.  This normalization is often called the $t$-score.  For the two independent samples case like what we have for comparing action movies to romance movies, the formula is $$T =\dfrac{ (\bar{x}_1 - \bar{x}_2) - (\mu_1 - \mu_2)}{ \sqrt{\dfrac{{s_1}^2}{n_1} + \dfrac{{s_2}^2}{n_2}}  }$$
  
There is a lot to try to unpack here.  

- $\bar{x}_1$ is the sample mean response of the first group
- $\bar{x}_2$ is the sample mean response of the second group
- $\mu_1$ is the population mean response of the first group
- $\mu_2$ is the population mean response of the second group
- $s_1$ is the sample standard deviation of the response of the first group
- $s_2$ is the sample standard deviation of the response of the second group
- $n_1$ is the sample size of the first group
- $n_2$ is the sample size of the second group
  
Assuming that the null hypothesis is true ($H_0: \mu_1 - \mu_2 = 0$), $T$ is said to be distributed following a $t$ distribution with degrees of freedom equal to the smaller value of $n_1 - 1$ and $n_2 - 1$.  The "degrees of freedom" can be thought of measuring how different the $t$ distribution will be as compared to a normal distribution.  Small sample sizes lead to small degrees of freedom and, thus, $t$ distributions that have more values in the tails of their distributions.  Large sample sizes lead to large degrees of freedom and, thus, $t$ distributions that closely align with the standard normal, bell-shaped curve. 
  
So, assuming $H_0$ is true, our formula simplifies a bit:

$$T =\dfrac{ \bar{x}_1 - \bar{x}_2}{ \sqrt{\dfrac{{s_1}^2}{n_1} + \dfrac{{s_2}^2}{n_2}}  }.$$
  
We have already built an approximation for what we think the distribution of $\delta = \bar{x}_1 - \bar{x}_2$ looks like using randomization above.  Recall this distribution:

```{r fig.cap="Simulated differences in means histogram"}
ggplot(data = rand_distn, aes(x = diffmean)) +
  geom_histogram(color = "white", bins = 20)
```

If we'd like to have a guess as to what the distribution of $T$ might look like instead, we need only to divide every value in `rand_distn` by $$\sqrt{\dfrac{{s_1}^2}{n_1} + \dfrac{{s_2}^2}{n_2}}.$$  As we did before, we will assign Romance to be group 1 and Action to be group 2.  (This was done since Romance comes second alphabetically and the reason why we have a number mismatch below with 1 and 2.) Remember that we've already calculated these values:

```{r}
kable(summary_ratings)
```

We will create some shortcuts here so you can see the value being calculated for the denominator of $T$.

```{r}
s1 <- summary_ratings$std_dev[2]
s2 <- summary_ratings$std_dev[1]
n1 <- summary_ratings$n[2]
n2 <- summary_ratings$n[1]
```

Here, we have $s_1 = `r s1`$, 
$s_2 = `r s2`$,
$n_1 = `r n1`$, and
$n_2 = `r n2`$.

We can calculate the denominator via

```{r}
(denom_T <- sqrt( (s1^2 / n1) + (s2^2 / n2) ))
```

Now if we divide all of the values of `diffmean` in `rand_distn` by `denom_T` we can have a simulated distribution of $T$ test statistics instead:

```{r, fig.cap="Simulated T statistics histogram"}
rand_distn <- rand_distn %>% 
  mutate(t_stat = diffmean / denom_T)
ggplot(data = rand_distn, aes(x = t_stat)) +
  geom_histogram(color = "white", bins = 20)
```

We see that the shape of this distribution is the same as that of `diffmean`.  The scale has changed though with `t_stat` having less spread than `diffmean`.

A traditional $t$-test doesn't look at this simulated distribution, but instead it looks at the $t$-curve with degrees of freedom equal to 33 (the minimum of $n_1 = 34 - 1 = 33$ and $n_2 = 34 - 1 = 33$).  This curve is frequently called a *density* curve and this is the reason why we specify the use of `y = ..density..` here in the `geom_histogram`.  We now overlay what this $t$-curve looks like on top of the histogram showing the simulated $T$ statistics.

```{r}
ggplot(data = rand_distn, mapping = aes(x = t_stat)) +
  geom_histogram(aes(y = ..density..), color = "white", binwidth = 0.3) +
  stat_function(fun = dt,
    args = list(df = min(n1 - 1, n2 - 1)), 
    color = "royalblue", size = 2)
```

We can see that the curve does a good job of approximating the randomization distribution here. (More on when to expect for this to be the case when we discuss conditions for the $t$-test in a bit.) To calculate the $p$-value in this case, we need to figure out how much of the total area under the $t$-curve is at our observed $T$-statistic or more, plus also adding the area under the curve at the negative value of the observed $T$-statistic or below.  (Remember this is a two-tailed test so we are looking for a difference--values in the tails of either direction.)  Just as we converted all of the simulated values to $T$-statistics, we must also do so for our observed effect $\delta^*$:

```{r}
(t_obs <- obs_diff / denom_T)
```

So graphically we are interested in finding the percentage of values that are at or above `r t_obs` or at or below `r -t_obs`.

```{r}
ggplot(data = rand_distn, mapping = aes(x = t_stat)) +
  stat_function(fun = dt,
    args = list(df = min(n1 - 1, n2 - 1)), 
    color = "royalblue", size = 2) +
  geom_vline(xintercept = t_obs, color = "red") +
  geom_vline(xintercept = -t_obs, color = "red")
```

At this point, you should make a guess as to what a reasonable value may be for the $p$-value.  Let's say the $p$-value is 0.01 or so.  To actually perform this calculation by hand, you'd need to do some calculus.  Let's have R do it for us instead using the `pt` function.

```{r}
pt(t_obs, df = min(n1 - 1, n2 - 1), lower.tail = FALSE) +
  pt(-t_obs, df = min(n1 - 1, n2 - 1), lower.tail = TRUE)
```

### Conditions for t-test

In order for the results of the $t$-test to be valid, three conditions must be met:

1. Independent observations in both samples
2. Nearly normal populations OR large sample sizes ($n \ge 30$)
3. Independently selected samples

Condition 1:  This is met since we sampled at random using R from our population.

Condition 2:  Recall from Figure \@ref(fig:movie-hist), that we know how the populations are distributed.  Both of them are close to normally distributed.  If we are a little concerned about this assumption, we also do have samples of size larger than 30 ($n_1 = n_2 = 34$).

Condition 3:  This is met since there is no natural pairing of a movie in the Action group to a movie in the Romance group.

Since all three conditions are met, we can be reasonably certain that the theory-based test will match the results of the randomization-based test using shuffling.  Remember that theory-based tests can produce some incorrect results in these assumptions are not carefully checked.  The only assumption for randomization and computational-based methods is that the sample is selected at random.  They are our preference and we strongly believe they should be yours as well, but it's important to also see how the theory-based tests can be done and used as an approximation for the computational techniques until at least more researchers are using these techniques that utilize the power of computers.

## Conclusion

### Script of R code

```{r include=FALSE, eval=FALSE, purl=FALSE}
knitr::purl("07-hypo.Rmd", "docs/scripts/07-hypo.R")
```

An R script file of all R code used in this chapter is available [here](http://ismayc.github.io/moderndiver-book/scripts/07-hypo.R).

    
### What's to come?

This chapter examined the basics of hypothesis testing with terminology and also an example of how to apply the "There is Only One Test" diagram to the Lady Tasting Tea example presented in Chapter \@ref(sim) and to an example on comparing the IMDB ratings of action movies and romance movies.  We'll see in Chapter \@ref(ci) how we can provide a range of possible values for an unknown population parameter instead of just running a Yes/No decision from a hypothesis test.
 
We will see in Chapter \@ref(regress) many of the same ideas we have seen with hypothesis testing and confidence intervals in the last two chapters.  Regression is frequently associated both correctly and incorrectly with statistics and data analysis, so you'll need to make sure you understand when it is appropriate and when it is not.
